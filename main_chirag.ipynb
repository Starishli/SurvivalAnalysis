{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 100)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# import viz\n",
    "import torch\n",
    "from torch import nn\n",
    "import survival_analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import network\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# event_col is the header in the df that represents the 'Event / Status' indicator\n",
    "# time_col is the header in the df that represents the event time\n",
    "def dataframe_to_deepsurv_ds(df, event_col = 'fstat', time_col = 'lenfol'):\n",
    "    # Extract the event and time columns as numpy arrays\n",
    "    e = df[event_col].values.astype(np.int32)\n",
    "    t = df[time_col].values.astype(np.float32)\n",
    "\n",
    "    # Extract the patient's covariates as a numpy array\n",
    "    x_df = df.drop([event_col, time_col], axis = 1)\n",
    "    x = x_df.values.astype(np.float32)\n",
    "    \n",
    "    # Return the deep surv dataframe\n",
    "    return {\n",
    "        'x' : x,\n",
    "        'e' : e,\n",
    "        't' : t\n",
    "    }\n",
    "\n",
    "class MyTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, standardize=False):\n",
    "        self.train_df = dataframe\n",
    "#         print(len(self.train_df.index))\n",
    "        # If the headers of the csv change, you can replace the values of \n",
    "        # 'event_col' and 'time_col' with the names of the new headers\n",
    "        # You can also use this function on your training dataset, validation dataset, and testing dataset\n",
    "        train_data = dataframe_to_deepsurv_ds(self.train_df, event_col = 'fstat', time_col= 'lenfol')\n",
    "\n",
    "        self.x, self.e, self.t = train_data['x'], train_data['e'], train_data['t']\n",
    "        \n",
    "        if standardize:\n",
    "            offset = self.x.mean(axis = 0)\n",
    "            scale = self.x.std(axis = 0)\n",
    "            self.x = (self.x - offset) / scale\n",
    "        \n",
    "        # Sort Training Data for Accurate Likelihood\n",
    "        sort_idx = np.argsort(self.t)[::-1]\n",
    "        self.x = self.x[sort_idx]\n",
    "        self.e = self.e[sort_idx]\n",
    "        self.t = self.t[sort_idx]\n",
    "        \n",
    "        self.processed_count = 1\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.train_df.index)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        self.processed_count += 1\n",
    "        return self.x[i], self.e[i], self.t[i]\n",
    "#         return (torch.from_numpy(x), torch.from_numpy(e), torch.from_numpy(t))\n",
    "\n",
    "ds = pd.read_csv('whas500.csv',sep=',')\n",
    "train = ds[:400]\n",
    "validation = ds[400:]\n",
    "train_ds = MyTrainDataset(train,True)\n",
    "validation_ds = MyTrainDataset(validation,True)\n",
    "\n",
    "# train_loader = dataloader.DataLoader(train_ds, shuffle=False, batch_size=len(train_ds))\n",
    "# validation_loader = dataloader.DataLoader(validation_ds, shuffle=False, batch_size=len(validation_ds))\n",
    "print(len(train_ds),len(validation_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds[['age', 'gender', 'bmi', 'miord']]\n",
    "e = ds['fstat']\n",
    "t = ds['lenfol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x.as_matrix())\n",
    "e = torch.from_numpy(e.as_matrix())\n",
    "t = torch.from_numpy(t.as_matrix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[83.0000,  0.0000, 25.5405,  1.0000],\n",
       "        [49.0000,  0.0000, 24.0240,  0.0000],\n",
       "        [70.0000,  1.0000, 22.1429,  0.0000],\n",
       "        ...,\n",
       "        [57.0000,  1.0000, 42.1358,  0.0000],\n",
       "        [67.0000,  0.0000, 27.4091,  0.0000],\n",
       "        [98.0000,  0.0000, 19.4857,  0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CUDA:\n",
    "    \n",
    "    x = x.cuda()\n",
    "    \n",
    "    e = e.cuda()\n",
    "    t = t.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the dataset to \"DeepSurv\" format\n",
    "DeepSurv expects a dataset to be in the form:\n",
    "\n",
    "    {\n",
    "        'x': numpy array of float32\n",
    "        'e': numpy array of int32\n",
    "        't': numpy array of float32\n",
    "        'hr': (optional) numpy array of float32\n",
    "    }\n",
    "    \n",
    "You are providing me a csv, which I read in as a pandas dataframe. Then I convert the pandas dataframe into the DeepSurv dataset format above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        #torch.nn.init.xavier_normal_(m.weight.data)\n",
    "        m.weight.data.fill_(0)\n",
    "def init_weights_for_cox(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(0)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPH model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b7a41bf91b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# If you have validation data, you can add it as the valid_dataloader parameter to the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# For CPH, set cox argument as True\n",
    "print(\"CPH model\")\n",
    "\n",
    "n_in = x.shape[1]\n",
    "\n",
    "#my_network = #network.DeepSurv(n_in, hidden_layers_sizes=[], dropout=dropout, batch_norm=batch_norm, momentum=0.1, cox=True)\n",
    "#my_network.apply(init_weights_for_cox)\n",
    "# network.load_state_dict(torch.load(\"model_99.pt\"))\n",
    "\n",
    "#optimizer = optimizer = torch.optim.SGD(my_network.parameters(), lr=learning_rate, momentum=momentum, weight_decay=L2_reg, nesterov=True)\n",
    "\n",
    "my_network = torch.nn.Sequential(torch.nn.Linear(n_in, 1))\n",
    "\n",
    "optimizer = torch.optim.Adam(my_network.parameters(), lr=1e-4)\n",
    "my_network.train()\n",
    "\n",
    "my_network.to(device)\n",
    "\n",
    "# If you have validation data, you can add it as the valid_dataloader parameter to the function\n",
    "metrics = survival_analysis.train(my_network, train_loader, device, optimizer, validation_loader, n_epochs,True)\n",
    "print()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepSurv(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train C-Index:', [0.6441259465275381, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805, 0.6447755740067805])\n"
     ]
    }
   ],
   "source": [
    "# Print the final metrics\n",
    "print('Train C-Index:', metrics['c-index'])\n",
    "# print('Valid C-Index: ',metrics['valid_c-index'][-1])\n",
    "\n",
    "# Plot the training / validation curves\n",
    "# viz.plot_log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
