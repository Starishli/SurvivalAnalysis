{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from lifelines.utils import concordance_index \n",
    "import sys\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import network\n",
    "from torch.utils.data import TensorDataset, Dataset\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import _pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset_dict={1:\"WHAS\",2:\"GBSG\",3:\"METABRIC\",4:\"SUPPORT\"}\n",
    "# DATASET_CHOICE = 1 # sys.argv[1]\n",
    "# dataset_name=dataset_dict[DATASET_CHOICE]\n",
    "\n",
    "def get_dataset(DATASET_CHOICE):\n",
    "    if (DATASET_CHOICE == 1):\n",
    "        # WHAS\n",
    "        ds = pd.read_csv('./datasets/whas1638.csv',sep=',')\n",
    "        train = ds[:1310]\n",
    "        valid = train[-100:]\n",
    "        train = train[:-100]\n",
    "        test = ds[1310:]\n",
    "        x_train = train[['0','1', '2', '3', '4', '5']].as_matrix()\n",
    "        x_valid = valid[['0','1', '2', '3', '4', '5']].as_matrix()\n",
    "        x_test = test[['0','1', '2', '3', '4', '5']].as_matrix() \n",
    "        name=\"WHAS\"\n",
    "    elif (DATASET_CHOICE == 2):\n",
    "        # GBSG\n",
    "        ds = pd.read_csv('./datasets/gbsg2232.csv',sep=',')\n",
    "        train = ds[:1546]\n",
    "        valid = train[-100:]\n",
    "        train = train[:-100]\n",
    "        test = ds[1546:]\n",
    "        x_train = train[['0','1', '2', '3', '4', '5', '6']].as_matrix()\n",
    "        x_valid = valid[['0','1', '2', '3', '4', '5', '6']].as_matrix()\n",
    "        x_test = test[['0','1', '2', '3', '4', '5', '6']].as_matrix() \n",
    "        name=\"GBSG\"\n",
    "    elif (DATASET_CHOICE == 3):\n",
    "        # for METABRIC\n",
    "        ds = pd.read_csv('./datasets/metabric1904.csv',sep=',')\n",
    "        train = ds[:1523]\n",
    "        valid = train[-100:]\n",
    "        train = train[:-100]\n",
    "        test = ds[1523:]\n",
    "        x_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix()\n",
    "        x_valid = valid[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix()\n",
    "        x_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8']].as_matrix() \n",
    "        name=\"METABRIC\"\n",
    "    elif (DATASET_CHOICE == 4):\n",
    "        # for SUPPORT\n",
    "        ds = pd.read_csv('./datasets/support8873.csv',sep=',')\n",
    "        train = ds[:7098]\n",
    "        valid = train[-100:]\n",
    "        train = train[:-100]\n",
    "        test = ds[7098:]\n",
    "        x_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', ]].as_matrix()\n",
    "        x_valid = valid[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix()\n",
    "        x_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']].as_matrix()\n",
    "        name=\"SUPPORT\"\n",
    "    return x_train,train,x_valid,valid,x_test,test,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data_to_torch(x_train,train,x_valid,valid,x_test,test):\n",
    "    scl = StandardScaler()\n",
    "    x_train = scl.fit_transform(x_train)\n",
    "    e_train = train['fstat']\n",
    "    t_train = train['lenfol']\n",
    "\n",
    "    x_valid = scl.fit_transform(x_valid)\n",
    "    e_valid = valid['fstat']\n",
    "    t_valid = valid['lenfol']\n",
    "\n",
    "    x_test = scl.transform(x_test)\n",
    "    e_test = test['fstat']\n",
    "    t_test = test['lenfol']\n",
    "\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    e_train = torch.from_numpy(e_train.as_matrix()).float()\n",
    "    t_train = torch.from_numpy(t_train.as_matrix())\n",
    "\n",
    "    x_valid = torch.from_numpy(x_valid).float()\n",
    "    e_valid = torch.from_numpy(e_valid.as_matrix()).float()\n",
    "    t_valid = torch.from_numpy(t_valid.as_matrix())\n",
    "\n",
    "\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    e_test = torch.from_numpy(e_test.as_matrix()).float()\n",
    "    t_test = torch.from_numpy(t_test.as_matrix())\n",
    "    \n",
    "    return x_train,e_train,t_train,x_valid,e_valid,t_valid,x_test,e_test,t_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_set(t_train,t_valid,t_test):\n",
    "    t_ = t_train.cpu().data.numpy()\n",
    "    risk_set = []\n",
    "    for i in range(len(t_)):\n",
    "\n",
    "        risk_set.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "    t_ = t_valid.cpu().data.numpy()\n",
    "\n",
    "    risk_set_valid = []\n",
    "    for i in range(len(t_)):\n",
    "\n",
    "        risk_set_valid.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "\n",
    "\n",
    "    t_ = t_test.cpu().data.numpy()\n",
    "\n",
    "    risk_set_test = []\n",
    "    for i in range(len(t_)):\n",
    "\n",
    "        risk_set_test.append([i]+np.where(t_>t_[i])[0].tolist())\n",
    "    return risk_set,risk_set_valid,risk_set_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elbo(risk, gated_output, E, risk_set):\n",
    "    go_sm = nn.Softmax(dim=1)(gated_output)\n",
    "    lnumerator = torch.mul(go_sm, risk)\n",
    "    lnumerator = torch.sum(lnumerator, dim=1)\n",
    "    expected_risks = torch.exp(risk) * go_sm\n",
    "    expected_risks = torch.sum(expected_risks, dim=1)\n",
    "    ldenominator = []\n",
    "    for i in range(risk.shape[0]):\n",
    "        ldenominator.append(torch.sum(expected_risks[risk_set[i]], dim=0))\n",
    "    ldenominator = torch.stack(ldenominator, dim=0)\n",
    "    ldenominator = torch.log(ldenominator)\n",
    "    likelihoods = lnumerator - ldenominator\n",
    "    E =  np.where(E.cpu().data.numpy()==1)[0]\n",
    "#     neg_likelihood = - torch.sum(likelihoods[E])\n",
    "    likelihoods = likelihoods[E]\n",
    "    neg_likelihood = - torch.sum(likelihoods)\n",
    "    \n",
    "    return neg_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_concordance_index(x, gated_x, t, e, bootstrap=False):\n",
    "    t = t.detach().cpu().numpy()\n",
    "    e = e.detach().cpu().numpy()\n",
    "    softmax = torch.nn.Softmax(dim=1)(gated_x)\n",
    "    r = x.shape[0]\n",
    "    soft_computed_hazard = torch.exp(x)\n",
    "    hard_computed_hazard = soft_computed_hazard[range(r),gated_x.argmax(1)[1]]\n",
    "    soft_computed_hazard = torch.mul(softmax, soft_computed_hazard)\n",
    "    soft_computed_hazard = torch.sum(soft_computed_hazard, dim = 1)\n",
    "    soft_computed_hazard = -1*soft_computed_hazard.detach().cpu().numpy()\n",
    "    hard_computed_hazard = -1*hard_computed_hazard.detach().cpu().numpy()\n",
    "    if not bootstrap:\n",
    "        return concordance_index(t,soft_computed_hazard,e),concordance_index(t,hard_computed_hazard,e)\n",
    "    else:\n",
    "        soft_concord, hard_concord = [], []\n",
    "        for i in range(bootstrap):\n",
    "            soft_dat_, e_, t_ = resample(soft_computed_hazard, e, t,random_state=i )       \n",
    "            sci = concordance_index(t_,soft_dat_,e_)\n",
    "            hard_dat_,  e_, t_  = resample(hard_computed_hazard,  e, t ,random_state=i)\n",
    "            hci = concordance_index(t_,hard_dat_,e_)\n",
    "            soft_concord.append(sci)\n",
    "            hard_concord.append(hci)\n",
    "        return soft_concord, hard_concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(gated_network, betas_network, risk_set, x_train, e_train, t_train, risk_set_valid, x_valid, e_valid, t_valid, \n",
    "          optimizer, n_epochs,x_test,e_test,t_test,risk_set_test):\n",
    "    # Initialize Metrics\n",
    "    c_index_soft = []\n",
    "    c_index_hard = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    test_loss = []\n",
    "    test_c_index_soft = []\n",
    "    test_c_index_hard = []\n",
    "    diff = 1e-4\n",
    "    prev_loss_train = 0\n",
    "    prev_loss_valid = 0\n",
    "    bad_cnt = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        gated_network.train()\n",
    "        betas_network.train()\n",
    "        optimizer.zero_grad()\n",
    "        gated_outputs = gated_network(x_train)\n",
    "        lsoftmax = nn.LogSoftmax(dim=1)(gated_outputs)\n",
    "        betas_output = betas_network(x_train)\n",
    "        ci_train_soft,ci_train_hard = get_concordance_index(betas_output, gated_outputs, t_train, e_train, bootstrap=False)\n",
    "        c_index_soft.append(ci_train_soft)\n",
    "        c_index_hard.append(ci_train_hard)\n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss = elbo(betas_output, gated_outputs, e_train, risk_set) + (betas_network[0].weight**2).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        my_loss = loss.cpu().data.numpy()\n",
    "        train_loss.append(my_loss)\n",
    "        if abs(my_loss - prev_loss_train) < diff:\n",
    "            break\n",
    "        prev_loss_train = my_loss\n",
    "        torch.cuda.empty_cache()         \n",
    "        ################################################# Validation #######################################################\n",
    "        gated_network.eval()\n",
    "        betas_network.eval()\n",
    "        gated_outputs_valid = gated_network(x_valid)\n",
    "        lsoftmax_valid = nn.LogSoftmax(dim=1)(gated_outputs_valid)\n",
    "        betas_output_valid = betas_network(x_valid)\n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "        loss_valid = elbo(betas_output_valid, gated_outputs_valid, e_valid, risk_set_valid)\n",
    "        my_loss_valid = loss_valid.cpu().data.numpy()\n",
    "        valid_loss.append(my_loss_valid)\n",
    "        if my_loss_valid - prev_loss_valid > diff:\n",
    "            bad_cnt+=1\n",
    "            if bad_cnt>2:\n",
    "                break\n",
    "        else:\n",
    "            bad_cnt=0\n",
    "        prev_loss_valid = my_loss_valid\n",
    "        torch.cuda.empty_cache()         \n",
    "\n",
    "    ################################################# Test #############################################################\n",
    "    gated_network.eval()\n",
    "    betas_network.eval()\n",
    "    gated_outputs_test = gated_network(x_test)\n",
    "    lsoftmax_test = nn.LogSoftmax(dim=1)(gated_outputs_test)\n",
    "    betas_output_test = betas_network(x_test)\n",
    "    ci_test_soft,ci_test_hard = get_concordance_index(betas_output_test, gated_outputs_test, t_test, e_test, bootstrap=250)\n",
    "    test_c_index_soft.append(ci_test_soft)\n",
    "    test_c_index_hard.append(ci_test_hard)\n",
    "#         loss = negative_log_likelihood(gated_outputs, betas_output, e, risk_set, CUDA)\n",
    "    loss_test = elbo(betas_output_test, gated_outputs_test, e_test, risk_set_test)\n",
    "    my_loss_test = loss_test.cpu().data.numpy()\n",
    "    test_loss.append(my_loss_test)\n",
    "    torch.cuda.empty_cache()         \n",
    "    \n",
    "    print('Finished training with %d epochs in %0.2fs' % (epoch + 1, time.time() - start))\n",
    "    metrics = {}\n",
    "    metrics['train_loss'] = train_loss\n",
    "    metrics['valid_loss'] = valid_loss\n",
    "    metrics['c-index-soft'] = c_index_soft\n",
    "    metrics['c-index-hard'] = c_index_hard\n",
    "    metrics['test_loss'] = test_loss\n",
    "    metrics['c-index-test-soft'] = test_c_index_soft\n",
    "    metrics['c-index-test-hard'] = test_c_index_hard\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(linear_model, learning_rate, layers_size, seed,data_dict):\n",
    "    layers_size = layers_size[:]\n",
    "    layers_size += [linear_model]\n",
    "    torch.manual_seed(seed)\n",
    "    n_in = data_dict[\"x_train\"].shape[1]\n",
    "    betas_network = nn.Sequential(nn.Linear(n_in, linear_model, bias=False) )\n",
    "    layers = []\n",
    "    for i in range(len(layers_size)-2):\n",
    "        layers.append(nn.Linear(layers_size[i],layers_size[i+1],bias=False ))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(layers_size[-2], layers_size[-1], bias=False))\n",
    "    gated_network = nn.Sequential(*layers)\n",
    "    optimizer = torch.optim.Adam(list(gated_network.parameters()) + list(betas_network.parameters()), lr=learning_rate)\n",
    "#     print(layers)\n",
    "#     if CUDA:\n",
    "#         gated_network.cuda()\n",
    "#         betas_network.cuda()\n",
    "    n_epochs = 2\n",
    "    metrics = train_model(gated_network, betas_network, \n",
    "                          data_dict[\"risk_set\"], data_dict[\"x_train\"], data_dict[\"e_train\"], data_dict[\"t_train\"],\n",
    "                        data_dict[\"risk_set_valid\"], data_dict[\"x_valid\"], data_dict[\"e_valid\"], data_dict[\"t_valid\"], \n",
    "                          optimizer, n_epochs,\n",
    "                          data_dict[\"x_test\"],data_dict[\"e_test\"],data_dict[\"t_test\"],data_dict[\"risk_set_test\"])   \n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk sets calculated\n"
     ]
    }
   ],
   "source": [
    "print(\"Risk sets calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_dict={1:\"WHAS\",2:\"GBSG\",3:\"METABRIC\",4:\"SUPPORT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_experiments():\n",
    "    for i in range(2,5):\n",
    "        x_train,train,x_valid,valid,x_test,test,name=get_dataset(i)\n",
    "        print(\"Running for\", name ,\"dataset\")\n",
    "        \n",
    "        x_train,e_train,t_train,x_valid,e_valid,t_valid,x_test,e_test,t_test = scale_data_to_torch(x_train,train,x_valid,valid,x_test,test)\n",
    "        print(\"Dataset loaded and scaled\")\n",
    "        \n",
    "        risk_set,risk_set_valid,risk_set_test=compute_risk_set(t_train,t_valid,t_test)\n",
    "        print(\"Risk set computed\")\n",
    "        \n",
    "        data_dict={\"x_train\":x_train,\"e_train\":e_train,\"t_train\":t_train,\n",
    "                   \"x_valid\":x_valid,\"e_valid\":e_valid,\"t_valid\":t_valid,\n",
    "                   \"x_test\":x_test,\"e_test\":e_test,\"t_test\":t_test,\n",
    "                   \"risk_set\":risk_set,\"risk_set_valid\":risk_set_valid,\"risk_set_test\":risk_set_test\n",
    "                  }\n",
    "        \n",
    "        n_in = x_train.shape[1]\n",
    "        linear_models=[2,5,10,12]\n",
    "        learning_rates=[1e-4,1e-3]\n",
    "        layer_sizes = [[n_in],[n_in,n_in],[n_in,n_in,n_in],[n_in,20,15]]\n",
    "        data=[data_dict]\n",
    "        hyperparams = [(linear_model, learning_rate, layer_size, seed, d) for layer_size in layer_sizes for learning_rate in learning_rates \n",
    "               for linear_model in linear_models for seed in range(3) for d in data]\n",
    "        print(\"Hyperparams initialized\")\n",
    "\n",
    "#         p = Pool(50)\n",
    "#         output = p.map(run_experiment, hyperparams)\n",
    "#         p.close()\n",
    "#         p.join()\n",
    "        a,b,c,d,e=hyperparams[0]\n",
    "        output=run_experiment(a,b,c,d,e)\n",
    "        print(\"Models trained. Writing to file\")\n",
    "        filename=name+\"_results\"\n",
    "        f = open(filename, \"wb\")\n",
    "        pkl.dump(output, f)\n",
    "        f.flush()\n",
    "        f.close()\n",
    "        print(name,\"done\")\n",
    "        print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for GBSG dataset\n",
      "Dataset loaded and scaled\n",
      "Risk set computed\n",
      "Hyperparams initialized\n",
      "Finished training with 2 epochs in 6.13s\n",
      "Models trained. Writing to file\n",
      "GBSG done\n",
      "\n",
      "Running for METABRIC dataset\n",
      "Dataset loaded and scaled\n",
      "Risk set computed\n",
      "Hyperparams initialized\n",
      "Finished training with 2 epochs in 3.43s\n",
      "Models trained. Writing to file\n",
      "METABRIC done\n",
      "\n",
      "Running for SUPPORT dataset\n",
      "Dataset loaded and scaled\n",
      "Risk set computed\n",
      "Hyperparams initialized\n",
      "Finished training with 2 epochs in 19.86s\n",
      "Models trained. Writing to file\n",
      "SUPPORT done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
